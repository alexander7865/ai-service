import streamlit as st
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
import time
from dotenv import load_dotenv
load_dotenv()

def app():
    
    st.subheader("ğŸ’¬ ì¸ê³µì§€ëŠ¥ ì±—ë´‡ (GPT-3.5-TURBO)")
    st.write("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš” ì¹œì ˆí•˜ê²Œ ë‹µí•´ë“œë¦´ê»˜ìš”")

    if "openai_model" not in st.session_state:
        st.session_state["openai_model"] = "gpt-3.5-turbo-16k-0613"

    if "message" not in st.session_state:
        st.session_state.message = []

    for message in st.session_state.message:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    

    # ìœ ì €ì˜ input
    if prompt := st.chat_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•œê°€ìš”?"):
        # ìœ ì €ê°€ ë³´ë‚¸ ë‚´ìš©ì€ ë‚´ìš© ê·¸ëŒ€ë¡œ ì‚¬ëŒ ì•„ì´ì½˜ê³¼ í•¨ê»˜ í™”ë©´ì— ì¶œë ¥í•˜ê¸° 
        st.session_state.message.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
    
        # AIê°€ ë‹µë³€í•  ë‚´ìš©: ìœ ì € ì¸í’‹ì„ LLMì—ê²Œ ë³´ë‚´ì„œ ì‹¤í–‰ì‹œí‚¤ê³ , ë°›ì€ ë‹µë³€ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥í•˜ê¸° 
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            # ì—¬ê¸°ì„œë¶€í„´ ìœ„ì—ì„œ ë§Œë“  ë°±ì—”ë“œ ì½”ë“œê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤! 
            # LLM
            llm = ChatOpenAI()
            
            # Prompt
            custom_prompt = ChatPromptTemplate(
                messages=[
                    SystemMessagePromptTemplate.from_template(
                        "You are a nice and smart chatbot having a conversation with a human."
                    ),
                    MessagesPlaceholder(variable_name="chat_history"),
                    HumanMessagePromptTemplate.from_template("{question}"),
                ]
            )

            memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
            conversation = LLMChain(llm=llm, prompt=custom_prompt, verbose=True, memory=memory)
            
            # ì—¬ê¸°ì„œ {"question": prompt}ëŠ” ìœ ì €ì˜ ì¸í’‹ í”„ë¡¬í”„íŠ¸ë¥¼ LLMì—ê²Œ ë³´ë‚¸ë‹¤ëŠ” ëœ»
            result = conversation.invoke({"question": prompt})
            # ì‹¤ì‹œê°„ìœ¼ë¡œ í…ìŠ¤íŠ¸ê°€ ë¡œë”©ë˜ëŠ” íš¨ê³¼ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ê²°ê³¼ê°’ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìª¼ê°œì£¼ê¸°
            for chunk in result['text'].split():
                full_response += chunk + " "
                time.sleep(0.05)
                message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
        st.session_state.message.append({"role": "assistant", "content": full_response})